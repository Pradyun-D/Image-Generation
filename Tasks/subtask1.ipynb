{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12977935,"sourceType":"datasetVersion","datasetId":8214271},{"sourceId":12989146,"sourceType":"datasetVersion","datasetId":8221627},{"sourceId":565610,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":426983,"modelId":444015}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef show(path):\n    img_arr = np.array(Image.open(path))\n    plt.imshow(img_arr)\n    plt.axis('off')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:40:36.671559Z","iopub.execute_input":"2025-09-07T17:40:36.671716Z","iopub.status.idle":"2025-09-07T17:40:36.678566Z","shell.execute_reply.started":"2025-09-07T17:40:36.671700Z","shell.execute_reply":"2025-09-07T17:40:36.677737Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport os\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset,DataLoader\nfrom torchvision.transforms import transforms\n\nclass UnderWaterImages(Dataset):\n    def __init__(self,folder,transforms=None):\n        self.dir = folder\n        self.images = os.listdir(self.dir)\n        self.transforms = transforms\n    \n    def __len__(self):\n        return len(self.images)\n        \n    def __getitem__(self,idx):\n        img_name = self.images[idx]\n        img_path = os.path.join(self.dir,img_name)\n        img = Image.open(img_path).convert('RGB')\n\n        if self.transforms:\n            img = self.transforms(img)\n        return img","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:40:36.680134Z","iopub.execute_input":"2025-09-07T17:40:36.680406Z","iopub.status.idle":"2025-09-07T17:40:43.952836Z","shell.execute_reply.started":"2025-09-07T17:40:36.680383Z","shell.execute_reply":"2025-09-07T17:40:43.952266Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"transforms = transforms.Compose([\n    transforms.Resize(224),\n    transforms.CenterCrop(224),\n    transforms.ToTensor()\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:40:43.953426Z","iopub.execute_input":"2025-09-07T17:40:43.953694Z","iopub.status.idle":"2025-09-07T17:40:43.957420Z","shell.execute_reply.started":"2025-09-07T17:40:43.953676Z","shell.execute_reply":"2025-09-07T17:40:43.956709Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from torch.utils.data import random_split, DataLoader\ntrain_dataset = UnderWaterImages('/kaggle/input/underwaterimagestrain/Raw',transforms)\ndataset_size = len(train_dataset)\nval_size = int(0.2 * dataset_size)   \ntrain_size = dataset_size - val_size\ntrain_subset, val_subset = random_split(train_dataset, [train_size, val_size])\ntrain_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:40:43.958008Z","iopub.execute_input":"2025-09-07T17:40:43.958164Z","iopub.status.idle":"2025-09-07T17:40:43.996920Z","shell.execute_reply.started":"2025-09-07T17:40:43.958150Z","shell.execute_reply":"2025-09-07T17:40:43.996260Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ResidualBlock(nn.Module):\n    def __init__(self,in_channels,out_channels,stride=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels,out_channels,3,stride=stride,padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels,out_channels,3,stride=1,padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Identity()\n        if in_channels!=out_channels or stride!=1:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels,out_channels,1,stride=stride),\n                nn.BatchNorm2d(out_channels)\n            )\n            \n    def forward(self,x):\n        identity = self.shortcut(x)\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = out + identity\n        out = F.relu(out)\n        return out ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:40:43.997692Z","iopub.execute_input":"2025-09-07T17:40:43.997895Z","iopub.status.idle":"2025-09-07T17:40:44.003583Z","shell.execute_reply.started":"2025-09-07T17:40:43.997870Z","shell.execute_reply":"2025-09-07T17:40:44.003037Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"epochs = 200 # \nbest_val_loss = float('inf')\npatience = 10 \npatience_counter = 0\n\n\nfor epoch in range(1, epochs + 1):\n    train(epoch)\n    \n    \n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for data in val_loader:\n            data = data.to(device)\n            recon_batch, mu, logvar = model(data)\n            loss = model.loss_function(recon_batch, data, mu, logvar)\n            val_loss += loss.item()\n    \n    avg_val_loss = val_loss / len(val_loader.dataset)\n    print(f'====> Epoch: {epoch} Average validation loss: {avg_val_loss:.4f}')\n\n    \n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), 'best_model.pth')\n        print('Validation loss decreased. Saving model.')\n    else:\n        patience_counter += 1\n        print(f'Validation loss did not improve. Patience: {patience_counter}/{patience}')\n\n    if patience_counter >= patience:\n        print('Early stopping triggered!')\n        break ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch import optim\ndevice = torch.accelerator.current_accelerator() if torch.accelerator.is_available() else 'cpu'\nmodel = VAE().to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nlog_interval = 10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T16:32:15.342174Z","iopub.execute_input":"2025-09-07T16:32:15.342680Z","iopub.status.idle":"2025-09-07T16:32:15.691166Z","shell.execute_reply.started":"2025-09-07T16:32:15.342656Z","shell.execute_reply":"2025-09-07T16:32:15.690567Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def train(epoch):\n    model.train() #what this does\n    train_loss = 0\n    for batch_idx, data in enumerate(train_loader):\n        data = data.to(device)\n        optimizer.zero_grad()\n        recon_batch, mu, logvar = model(data)\n        loss = model.loss_function(recon_batch, data, mu, logvar)\n        loss.backward()\n        train_loss += loss.item()\n        optimizer.step()\n        if batch_idx % log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader),\n                loss.item() / len(data)))\n\n    print('====> Epoch: {} Average loss: {:.4f}'.format(\n          epoch, train_loss / len(train_loader.dataset)))\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T16:32:20.236226Z","iopub.execute_input":"2025-09-07T16:32:20.236499Z","iopub.status.idle":"2025-09-07T16:32:20.242318Z","shell.execute_reply.started":"2025-09-07T16:32:20.236479Z","shell.execute_reply":"2025-09-07T16:32:20.241725Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"epochs = 200 # \nbest_val_loss = float('inf')\npatience = 10 \npatience_counter = 0\n\n\nfor epoch in range(1, epochs + 1):\n    train(epoch)\n    \n    \n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for data in val_loader:\n            data = data.to(device)\n            recon_batch, mu, logvar = model(data)\n            loss = model.loss_function(recon_batch, data, mu, logvar)\n            val_loss += loss.item()\n    \n    avg_val_loss = val_loss / len(val_loader.dataset)\n    print(f'====> Epoch: {epoch} Average validation loss: {avg_val_loss:.4f}')\n\n    \n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), 'best_model.pth')\n        print('Validation loss decreased. Saving model.')\n    else:\n        patience_counter += 1\n        print(f'Validation loss did not improve. Patience: {patience_counter}/{patience}')\n\n    if patience_counter >= patience:\n        print('Early stopping triggered!')\n        break ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T13:27:47.118152Z","iopub.execute_input":"2025-09-07T13:27:47.118524Z","iopub.status.idle":"2025-09-07T13:51:19.511435Z","shell.execute_reply.started":"2025-09-07T13:27:47.118501Z","shell.execute_reply":"2025-09-07T13:51:19.510523Z"}},"outputs":[{"name":"stdout","text":"Train Epoch: 1 [0/560 (0%)]\tLoss: 317.402496\nTrain Epoch: 1 [320/560 (56%)]\tLoss: 210.643280\n====> Epoch: 1 Average loss: 260.4387\n====> Epoch: 1 Average validation loss: 252.8565\nValidation loss decreased. Saving model.\nTrain Epoch: 2 [0/560 (0%)]\tLoss: 230.124771\nTrain Epoch: 2 [320/560 (56%)]\tLoss: 208.772766\n====> Epoch: 2 Average loss: 206.4026\n====> Epoch: 2 Average validation loss: 214.4464\nValidation loss decreased. Saving model.\nTrain Epoch: 3 [0/560 (0%)]\tLoss: 172.069000\nTrain Epoch: 3 [320/560 (56%)]\tLoss: 196.074066\n====> Epoch: 3 Average loss: 184.0703\n====> Epoch: 3 Average validation loss: 197.1457\nValidation loss decreased. Saving model.\nTrain Epoch: 4 [0/560 (0%)]\tLoss: 202.848831\nTrain Epoch: 4 [320/560 (56%)]\tLoss: 199.888901\n====> Epoch: 4 Average loss: 178.7315\n====> Epoch: 4 Average validation loss: 194.2405\nValidation loss decreased. Saving model.\nTrain Epoch: 5 [0/560 (0%)]\tLoss: 151.678421\nTrain Epoch: 5 [320/560 (56%)]\tLoss: 160.720627\n====> Epoch: 5 Average loss: 173.1309\n====> Epoch: 5 Average validation loss: 190.5095\nValidation loss decreased. Saving model.\nTrain Epoch: 6 [0/560 (0%)]\tLoss: 182.556610\nTrain Epoch: 6 [320/560 (56%)]\tLoss: 163.777969\n====> Epoch: 6 Average loss: 172.1929\n====> Epoch: 6 Average validation loss: 184.8078\nValidation loss decreased. Saving model.\nTrain Epoch: 7 [0/560 (0%)]\tLoss: 146.378983\nTrain Epoch: 7 [320/560 (56%)]\tLoss: 164.887146\n====> Epoch: 7 Average loss: 166.0671\n====> Epoch: 7 Average validation loss: 178.5216\nValidation loss decreased. Saving model.\nTrain Epoch: 8 [0/560 (0%)]\tLoss: 151.476562\nTrain Epoch: 8 [320/560 (56%)]\tLoss: 143.221512\n====> Epoch: 8 Average loss: 157.1750\n====> Epoch: 8 Average validation loss: 171.6220\nValidation loss decreased. Saving model.\nTrain Epoch: 9 [0/560 (0%)]\tLoss: 155.549438\nTrain Epoch: 9 [320/560 (56%)]\tLoss: 152.415588\n====> Epoch: 9 Average loss: 147.9972\n====> Epoch: 9 Average validation loss: 165.0517\nValidation loss decreased. Saving model.\nTrain Epoch: 10 [0/560 (0%)]\tLoss: 138.039001\nTrain Epoch: 10 [320/560 (56%)]\tLoss: 154.181396\n====> Epoch: 10 Average loss: 139.5377\n====> Epoch: 10 Average validation loss: 152.5774\nValidation loss decreased. Saving model.\nTrain Epoch: 11 [0/560 (0%)]\tLoss: 153.833313\nTrain Epoch: 11 [320/560 (56%)]\tLoss: 126.609512\n====> Epoch: 11 Average loss: 133.7441\n====> Epoch: 11 Average validation loss: 154.5295\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 12 [0/560 (0%)]\tLoss: 113.828842\nTrain Epoch: 12 [320/560 (56%)]\tLoss: 115.318405\n====> Epoch: 12 Average loss: 131.7586\n====> Epoch: 12 Average validation loss: 143.7811\nValidation loss decreased. Saving model.\nTrain Epoch: 13 [0/560 (0%)]\tLoss: 139.471237\nTrain Epoch: 13 [320/560 (56%)]\tLoss: 125.462921\n====> Epoch: 13 Average loss: 130.6516\n====> Epoch: 13 Average validation loss: 144.1017\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 14 [0/560 (0%)]\tLoss: 127.610489\nTrain Epoch: 14 [320/560 (56%)]\tLoss: 133.372925\n====> Epoch: 14 Average loss: 130.1737\n====> Epoch: 14 Average validation loss: 150.7009\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 15 [0/560 (0%)]\tLoss: 132.570435\nTrain Epoch: 15 [320/560 (56%)]\tLoss: 144.118454\n====> Epoch: 15 Average loss: 129.8893\n====> Epoch: 15 Average validation loss: 138.9088\nValidation loss decreased. Saving model.\nTrain Epoch: 16 [0/560 (0%)]\tLoss: 137.781937\nTrain Epoch: 16 [320/560 (56%)]\tLoss: 133.802551\n====> Epoch: 16 Average loss: 126.4290\n====> Epoch: 16 Average validation loss: 138.8766\nValidation loss decreased. Saving model.\nTrain Epoch: 17 [0/560 (0%)]\tLoss: 135.575241\nTrain Epoch: 17 [320/560 (56%)]\tLoss: 124.563873\n====> Epoch: 17 Average loss: 125.6560\n====> Epoch: 17 Average validation loss: 133.0012\nValidation loss decreased. Saving model.\nTrain Epoch: 18 [0/560 (0%)]\tLoss: 127.081039\nTrain Epoch: 18 [320/560 (56%)]\tLoss: 137.076614\n====> Epoch: 18 Average loss: 124.2906\n====> Epoch: 18 Average validation loss: 139.0623\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 19 [0/560 (0%)]\tLoss: 123.312469\nTrain Epoch: 19 [320/560 (56%)]\tLoss: 111.451736\n====> Epoch: 19 Average loss: 122.6863\n====> Epoch: 19 Average validation loss: 129.9148\nValidation loss decreased. Saving model.\nTrain Epoch: 20 [0/560 (0%)]\tLoss: 107.837914\nTrain Epoch: 20 [320/560 (56%)]\tLoss: 96.852730\n====> Epoch: 20 Average loss: 120.0925\n====> Epoch: 20 Average validation loss: 130.2340\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 21 [0/560 (0%)]\tLoss: 136.332733\nTrain Epoch: 21 [320/560 (56%)]\tLoss: 116.162834\n====> Epoch: 21 Average loss: 121.0359\n====> Epoch: 21 Average validation loss: 129.6078\nValidation loss decreased. Saving model.\nTrain Epoch: 22 [0/560 (0%)]\tLoss: 119.140907\nTrain Epoch: 22 [320/560 (56%)]\tLoss: 132.805099\n====> Epoch: 22 Average loss: 121.5581\n====> Epoch: 22 Average validation loss: 129.0220\nValidation loss decreased. Saving model.\nTrain Epoch: 23 [0/560 (0%)]\tLoss: 127.257874\nTrain Epoch: 23 [320/560 (56%)]\tLoss: 126.603104\n====> Epoch: 23 Average loss: 119.3210\n====> Epoch: 23 Average validation loss: 127.1116\nValidation loss decreased. Saving model.\nTrain Epoch: 24 [0/560 (0%)]\tLoss: 106.387810\nTrain Epoch: 24 [320/560 (56%)]\tLoss: 98.792839\n====> Epoch: 24 Average loss: 117.6553\n====> Epoch: 24 Average validation loss: 130.4660\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 25 [0/560 (0%)]\tLoss: 114.256561\nTrain Epoch: 25 [320/560 (56%)]\tLoss: 104.147652\n====> Epoch: 25 Average loss: 118.1194\n====> Epoch: 25 Average validation loss: 138.2445\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 26 [0/560 (0%)]\tLoss: 111.649353\nTrain Epoch: 26 [320/560 (56%)]\tLoss: 134.980179\n====> Epoch: 26 Average loss: 118.1887\n====> Epoch: 26 Average validation loss: 138.4634\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 27 [0/560 (0%)]\tLoss: 120.948349\nTrain Epoch: 27 [320/560 (56%)]\tLoss: 104.121262\n====> Epoch: 27 Average loss: 116.5472\n====> Epoch: 27 Average validation loss: 125.8493\nValidation loss decreased. Saving model.\nTrain Epoch: 28 [0/560 (0%)]\tLoss: 132.782547\nTrain Epoch: 28 [320/560 (56%)]\tLoss: 106.544037\n====> Epoch: 28 Average loss: 116.1448\n====> Epoch: 28 Average validation loss: 124.7761\nValidation loss decreased. Saving model.\nTrain Epoch: 29 [0/560 (0%)]\tLoss: 129.456497\nTrain Epoch: 29 [320/560 (56%)]\tLoss: 129.542160\n====> Epoch: 29 Average loss: 114.5482\n====> Epoch: 29 Average validation loss: 125.7658\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 30 [0/560 (0%)]\tLoss: 102.789505\nTrain Epoch: 30 [320/560 (56%)]\tLoss: 104.596771\n====> Epoch: 30 Average loss: 113.9313\n====> Epoch: 30 Average validation loss: 126.6846\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 31 [0/560 (0%)]\tLoss: 109.156281\nTrain Epoch: 31 [320/560 (56%)]\tLoss: 117.920166\n====> Epoch: 31 Average loss: 112.9777\n====> Epoch: 31 Average validation loss: 122.1453\nValidation loss decreased. Saving model.\nTrain Epoch: 32 [0/560 (0%)]\tLoss: 113.666634\nTrain Epoch: 32 [320/560 (56%)]\tLoss: 99.063690\n====> Epoch: 32 Average loss: 111.8788\n====> Epoch: 32 Average validation loss: 120.5731\nValidation loss decreased. Saving model.\nTrain Epoch: 33 [0/560 (0%)]\tLoss: 99.337799\nTrain Epoch: 33 [320/560 (56%)]\tLoss: 118.303146\n====> Epoch: 33 Average loss: 112.5398\n====> Epoch: 33 Average validation loss: 123.1067\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 34 [0/560 (0%)]\tLoss: 108.823830\nTrain Epoch: 34 [320/560 (56%)]\tLoss: 112.184044\n====> Epoch: 34 Average loss: 111.4971\n====> Epoch: 34 Average validation loss: 119.4267\nValidation loss decreased. Saving model.\nTrain Epoch: 35 [0/560 (0%)]\tLoss: 96.197502\nTrain Epoch: 35 [320/560 (56%)]\tLoss: 102.913986\n====> Epoch: 35 Average loss: 110.2350\n====> Epoch: 35 Average validation loss: 120.3290\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 36 [0/560 (0%)]\tLoss: 104.688202\nTrain Epoch: 36 [320/560 (56%)]\tLoss: 113.987808\n====> Epoch: 36 Average loss: 109.9618\n====> Epoch: 36 Average validation loss: 121.5837\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 37 [0/560 (0%)]\tLoss: 105.313736\nTrain Epoch: 37 [320/560 (56%)]\tLoss: 119.894600\n====> Epoch: 37 Average loss: 108.9405\n====> Epoch: 37 Average validation loss: 117.9525\nValidation loss decreased. Saving model.\nTrain Epoch: 38 [0/560 (0%)]\tLoss: 101.446243\nTrain Epoch: 38 [320/560 (56%)]\tLoss: 102.962753\n====> Epoch: 38 Average loss: 109.0757\n====> Epoch: 38 Average validation loss: 118.1800\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 39 [0/560 (0%)]\tLoss: 112.702698\nTrain Epoch: 39 [320/560 (56%)]\tLoss: 86.731621\n====> Epoch: 39 Average loss: 109.7888\n====> Epoch: 39 Average validation loss: 119.3427\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 40 [0/560 (0%)]\tLoss: 102.367226\nTrain Epoch: 40 [320/560 (56%)]\tLoss: 91.064445\n====> Epoch: 40 Average loss: 108.4545\n====> Epoch: 40 Average validation loss: 121.4653\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 41 [0/560 (0%)]\tLoss: 123.442749\nTrain Epoch: 41 [320/560 (56%)]\tLoss: 102.180260\n====> Epoch: 41 Average loss: 106.6281\n====> Epoch: 41 Average validation loss: 117.0287\nValidation loss decreased. Saving model.\nTrain Epoch: 42 [0/560 (0%)]\tLoss: 106.460838\nTrain Epoch: 42 [320/560 (56%)]\tLoss: 105.381775\n====> Epoch: 42 Average loss: 107.4341\n====> Epoch: 42 Average validation loss: 115.8756\nValidation loss decreased. Saving model.\nTrain Epoch: 43 [0/560 (0%)]\tLoss: 116.081848\nTrain Epoch: 43 [320/560 (56%)]\tLoss: 86.803017\n====> Epoch: 43 Average loss: 106.5173\n====> Epoch: 43 Average validation loss: 118.6214\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 44 [0/560 (0%)]\tLoss: 110.379753\nTrain Epoch: 44 [320/560 (56%)]\tLoss: 92.796440\n====> Epoch: 44 Average loss: 105.5442\n====> Epoch: 44 Average validation loss: 117.0176\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 45 [0/560 (0%)]\tLoss: 108.416702\nTrain Epoch: 45 [320/560 (56%)]\tLoss: 99.565918\n====> Epoch: 45 Average loss: 105.5182\n====> Epoch: 45 Average validation loss: 116.1059\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 46 [0/560 (0%)]\tLoss: 102.633629\nTrain Epoch: 46 [320/560 (56%)]\tLoss: 90.431244\n====> Epoch: 46 Average loss: 105.6809\n====> Epoch: 46 Average validation loss: 117.7612\nValidation loss did not improve. Patience: 4/10\nTrain Epoch: 47 [0/560 (0%)]\tLoss: 80.943329\nTrain Epoch: 47 [320/560 (56%)]\tLoss: 114.533653\n====> Epoch: 47 Average loss: 103.9929\n====> Epoch: 47 Average validation loss: 116.8297\nValidation loss did not improve. Patience: 5/10\nTrain Epoch: 48 [0/560 (0%)]\tLoss: 110.286598\nTrain Epoch: 48 [320/560 (56%)]\tLoss: 91.547157\n====> Epoch: 48 Average loss: 107.2301\n====> Epoch: 48 Average validation loss: 116.8424\nValidation loss did not improve. Patience: 6/10\nTrain Epoch: 49 [0/560 (0%)]\tLoss: 98.027374\nTrain Epoch: 49 [320/560 (56%)]\tLoss: 93.598267\n====> Epoch: 49 Average loss: 104.6349\n====> Epoch: 49 Average validation loss: 112.3367\nValidation loss decreased. Saving model.\nTrain Epoch: 50 [0/560 (0%)]\tLoss: 122.518021\nTrain Epoch: 50 [320/560 (56%)]\tLoss: 84.715698\n====> Epoch: 50 Average loss: 103.7465\n====> Epoch: 50 Average validation loss: 113.1967\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 51 [0/560 (0%)]\tLoss: 98.871819\nTrain Epoch: 51 [320/560 (56%)]\tLoss: 106.800514\n====> Epoch: 51 Average loss: 104.6838\n====> Epoch: 51 Average validation loss: 112.2267\nValidation loss decreased. Saving model.\nTrain Epoch: 52 [0/560 (0%)]\tLoss: 90.105850\nTrain Epoch: 52 [320/560 (56%)]\tLoss: 95.086166\n====> Epoch: 52 Average loss: 103.7126\n====> Epoch: 52 Average validation loss: 113.4114\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 53 [0/560 (0%)]\tLoss: 100.474617\nTrain Epoch: 53 [320/560 (56%)]\tLoss: 121.115059\n====> Epoch: 53 Average loss: 105.2360\n====> Epoch: 53 Average validation loss: 117.6675\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 54 [0/560 (0%)]\tLoss: 101.677567\nTrain Epoch: 54 [320/560 (56%)]\tLoss: 97.695969\n====> Epoch: 54 Average loss: 103.7640\n====> Epoch: 54 Average validation loss: 113.5060\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 55 [0/560 (0%)]\tLoss: 107.291969\nTrain Epoch: 55 [320/560 (56%)]\tLoss: 113.670517\n====> Epoch: 55 Average loss: 103.2921\n====> Epoch: 55 Average validation loss: 115.1508\nValidation loss did not improve. Patience: 4/10\nTrain Epoch: 56 [0/560 (0%)]\tLoss: 99.033440\nTrain Epoch: 56 [320/560 (56%)]\tLoss: 97.814423\n====> Epoch: 56 Average loss: 102.6648\n====> Epoch: 56 Average validation loss: 110.3251\nValidation loss decreased. Saving model.\nTrain Epoch: 57 [0/560 (0%)]\tLoss: 92.723228\nTrain Epoch: 57 [320/560 (56%)]\tLoss: 105.549210\n====> Epoch: 57 Average loss: 101.9565\n====> Epoch: 57 Average validation loss: 111.4695\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 58 [0/560 (0%)]\tLoss: 95.784622\nTrain Epoch: 58 [320/560 (56%)]\tLoss: 110.051613\n====> Epoch: 58 Average loss: 103.6361\n====> Epoch: 58 Average validation loss: 109.5622\nValidation loss decreased. Saving model.\nTrain Epoch: 59 [0/560 (0%)]\tLoss: 108.599831\nTrain Epoch: 59 [320/560 (56%)]\tLoss: 94.451347\n====> Epoch: 59 Average loss: 103.1274\n====> Epoch: 59 Average validation loss: 111.2847\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 60 [0/560 (0%)]\tLoss: 115.459312\nTrain Epoch: 60 [320/560 (56%)]\tLoss: 87.432404\n====> Epoch: 60 Average loss: 102.2437\n====> Epoch: 60 Average validation loss: 111.6879\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 61 [0/560 (0%)]\tLoss: 99.700546\nTrain Epoch: 61 [320/560 (56%)]\tLoss: 84.155411\n====> Epoch: 61 Average loss: 101.8850\n====> Epoch: 61 Average validation loss: 109.6244\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 62 [0/560 (0%)]\tLoss: 100.632568\nTrain Epoch: 62 [320/560 (56%)]\tLoss: 103.881485\n====> Epoch: 62 Average loss: 101.3269\n====> Epoch: 62 Average validation loss: 108.8336\nValidation loss decreased. Saving model.\nTrain Epoch: 63 [0/560 (0%)]\tLoss: 101.299301\nTrain Epoch: 63 [320/560 (56%)]\tLoss: 97.578461\n====> Epoch: 63 Average loss: 103.5592\n====> Epoch: 63 Average validation loss: 110.7459\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 64 [0/560 (0%)]\tLoss: 86.400826\nTrain Epoch: 64 [320/560 (56%)]\tLoss: 101.367020\n====> Epoch: 64 Average loss: 100.9514\n====> Epoch: 64 Average validation loss: 112.6559\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 65 [0/560 (0%)]\tLoss: 88.473557\nTrain Epoch: 65 [320/560 (56%)]\tLoss: 95.029610\n====> Epoch: 65 Average loss: 101.2677\n====> Epoch: 65 Average validation loss: 111.5822\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 66 [0/560 (0%)]\tLoss: 92.562889\nTrain Epoch: 66 [320/560 (56%)]\tLoss: 99.048973\n====> Epoch: 66 Average loss: 101.7335\n====> Epoch: 66 Average validation loss: 106.8860\nValidation loss decreased. Saving model.\nTrain Epoch: 67 [0/560 (0%)]\tLoss: 94.639870\nTrain Epoch: 67 [320/560 (56%)]\tLoss: 89.851395\n====> Epoch: 67 Average loss: 99.0964\n====> Epoch: 67 Average validation loss: 109.0115\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 68 [0/560 (0%)]\tLoss: 96.241982\nTrain Epoch: 68 [320/560 (56%)]\tLoss: 97.865067\n====> Epoch: 68 Average loss: 99.2704\n====> Epoch: 68 Average validation loss: 109.8964\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 69 [0/560 (0%)]\tLoss: 97.469063\nTrain Epoch: 69 [320/560 (56%)]\tLoss: 105.777542\n====> Epoch: 69 Average loss: 99.9210\n====> Epoch: 69 Average validation loss: 108.7778\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 70 [0/560 (0%)]\tLoss: 96.788055\nTrain Epoch: 70 [320/560 (56%)]\tLoss: 111.950668\n====> Epoch: 70 Average loss: 100.5010\n====> Epoch: 70 Average validation loss: 109.2306\nValidation loss did not improve. Patience: 4/10\nTrain Epoch: 71 [0/560 (0%)]\tLoss: 98.125320\nTrain Epoch: 71 [320/560 (56%)]\tLoss: 103.347672\n====> Epoch: 71 Average loss: 99.2807\n====> Epoch: 71 Average validation loss: 109.4732\nValidation loss did not improve. Patience: 5/10\nTrain Epoch: 72 [0/560 (0%)]\tLoss: 87.253471\nTrain Epoch: 72 [320/560 (56%)]\tLoss: 109.667953\n====> Epoch: 72 Average loss: 99.7723\n====> Epoch: 72 Average validation loss: 110.9217\nValidation loss did not improve. Patience: 6/10\nTrain Epoch: 73 [0/560 (0%)]\tLoss: 99.137856\nTrain Epoch: 73 [320/560 (56%)]\tLoss: 108.614899\n====> Epoch: 73 Average loss: 100.6808\n====> Epoch: 73 Average validation loss: 110.2548\nValidation loss did not improve. Patience: 7/10\nTrain Epoch: 74 [0/560 (0%)]\tLoss: 90.902573\nTrain Epoch: 74 [320/560 (56%)]\tLoss: 102.133255\n====> Epoch: 74 Average loss: 100.2694\n====> Epoch: 74 Average validation loss: 108.5785\nValidation loss did not improve. Patience: 8/10\nTrain Epoch: 75 [0/560 (0%)]\tLoss: 93.450569\nTrain Epoch: 75 [320/560 (56%)]\tLoss: 93.734756\n====> Epoch: 75 Average loss: 100.2374\n====> Epoch: 75 Average validation loss: 110.1908\nValidation loss did not improve. Patience: 9/10\nTrain Epoch: 76 [0/560 (0%)]\tLoss: 100.136307\nTrain Epoch: 76 [320/560 (56%)]\tLoss: 109.239616\n====> Epoch: 76 Average loss: 97.7494\n====> Epoch: 76 Average validation loss: 108.3925\nValidation loss did not improve. Patience: 10/10\nEarly stopping triggered!\n","output_type":"stream"}],"execution_count":99},{"cell_type":"code","source":"import os\n\nfor root, dirs, files in os.walk(\"/kaggle/input\"):\n    for f in files:\n        if \"reconstruction_76.png\" in f:\n            print(os.path.join(root, f))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T18:59:55.903116Z","iopub.execute_input":"2025-09-07T18:59:55.903621Z","iopub.status.idle":"2025-09-07T18:59:56.997843Z","shell.execute_reply.started":"2025-09-07T18:59:55.903599Z","shell.execute_reply":"2025-09-07T18:59:56.997279Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/results/reconstruction_76.png\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"**Results: Check img1 on GITHUB**","metadata":{}},{"cell_type":"markdown","source":"**The blurry images is mostly because of:**\n1. The KL divergence is not correctly scaled down. Model is trying to squeeze in too much of info a limited gaussian\n2. The model's complexity can be increased a bit more. It might be finding it difficult to reconstruct the image back\n","metadata":{}},{"cell_type":"markdown","source":"**Experiment: Scale down the KL loss by multiplying by a beta factor**","metadata":{}},{"cell_type":"code","source":"class VAE(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        ##decoder\n        self.conv1 = nn.Conv2d(3,16,7,stride=2,padding=3)\n        \n        #each residual block halfs the dimension\n        self.res1 = ResidualBlock(16,32,2) \n        self.res2 = ResidualBlock(32,64,2)\n        self.res3 = ResidualBlock(64,128,2)\n        self.res4 = ResidualBlock(128,256,2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        self.fc_mean = nn.Linear(256,128)\n        self.fc_var = nn.Linear(256,128)\n        # self.fc_mean = nn.Linear(128,64)\n        # self.fc_var = nn.Linear(128,64)        \n\n        ##encoder\n        self.fc1 = nn.Linear(128,256*7*7)\n\n        #kernel=4 stride=2 padding=1 always doubles the dim\n        self.up1 = nn.ConvTranspose2d(256,128,4,stride=2,padding=1)\n        self.bn1 = nn.BatchNorm2d(128)\n        self.up2 = nn.ConvTranspose2d(128,64,4,stride=2,padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.up3 = nn.ConvTranspose2d(64,32,4,stride=2,padding=1)   \n        self.bn3 = nn.BatchNorm2d(32)\n        self.up4 = nn.ConvTranspose2d(32,16,4,stride=2,padding=1)  \n        self.bn4 = nn.BatchNorm2d(16)\n        self.up5 = nn.ConvTranspose2d(16,3,4,stride=2,padding=1)      \n\n        \n    def encode(self,x):\n        x = F.relu(self.conv1(x))\n        x = self.res1(x)\n        x = self.res2(x)\n        x = self.res3(x)\n        x = self.res4(x)\n        x = torch.flatten(self.avgpool(x),1)\n        mean = self.fc_mean(x)\n        log_var = self.fc_var(x)\n        return mean,log_var\n\n    def reparameterize(self,mu,log_var):\n        std = torch.exp(0.5*log_var) #0.5 prevents very huge value \n        eps = torch.randn_like(std)\n        z = mu + std*eps\n        return z\n\n    def decode(self,z):\n        out = F.relu(self.fc1(z))\n        out = out.view(-1,256,7,7)\n        out = F.relu(self.bn1(self.up1(out)))\n        out = F.relu(self.bn2(self.up2(out)))\n        out = F.relu(self.bn3(self.up3(out)))\n        out = F.relu(self.bn4(self.up4(out)))\n        out = torch.sigmoid(self.up5(out))\n        return out\n\n    def loss_function(self,out,x,mu,log_vars,beta=0.5):\n        loss = nn.MSELoss(reduction='mean')\n        recon_loss = loss(x,out)*(x.shape[1] * x.shape[2] * x.shape[3])\n        \n        kld = -0.5 * torch.sum(1 + log_vars - mu.pow(2) - log_vars.exp())\n\n        return recon_loss+(kld*beta)\n\n    def forward(self,x):\n        mu,log_vars = self.encode(x)\n        z = self.reparameterize(mu,log_vars)\n        out = self.decode(z)\n\n        return out,mu,log_vars","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T16:32:14.424511Z","iopub.execute_input":"2025-09-07T16:32:14.425049Z","iopub.status.idle":"2025-09-07T16:32:14.435671Z","shell.execute_reply.started":"2025-09-07T16:32:14.425024Z","shell.execute_reply":"2025-09-07T16:32:14.434904Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"epochs = 100\nbest_val_loss = float('inf')\npatience = 10 \npatience_counter = 0\n\n\nfor epoch in range(1, epochs + 1):\n    train(epoch)\n    \n    \n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for data in val_loader:\n            data = data.to(device)\n            recon_batch, mu, logvar = model(data)\n            loss = model.loss_function(recon_batch, data, mu, logvar)\n            val_loss += loss.item()\n    \n    avg_val_loss = val_loss / len(val_loader.dataset)\n    print(f'Epoch: {epoch} Average validation loss: {avg_val_loss:.4f}')\n\n    \n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), 'best_model.pth')\n        print('Validation loss decreased. Saving model.')\n    else:\n        patience_counter += 1\n        print(f'Validation loss did not improve. Patience: {patience_counter}/{patience}')\n\n    if patience_counter >= patience:\n        print('Early stopping triggered!')\n        break ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T16:33:44.129316Z","iopub.execute_input":"2025-09-07T16:33:44.129606Z","iopub.status.idle":"2025-09-07T17:03:11.987925Z","shell.execute_reply.started":"2025-09-07T16:33:44.129582Z","shell.execute_reply":"2025-09-07T17:03:11.987002Z"}},"outputs":[{"name":"stdout","text":"Train Epoch: 1 [0/560 (0%)]\tLoss: 473.049774\nTrain Epoch: 1 [320/560 (56%)]\tLoss: 316.546906\n====> Epoch: 1 Average loss: 343.7460\nEpoch: 1 Average validation loss: 391.1387\nValidation loss decreased. Saving model.\nTrain Epoch: 2 [0/560 (0%)]\tLoss: 308.861847\nTrain Epoch: 2 [320/560 (56%)]\tLoss: 253.811829\n====> Epoch: 2 Average loss: 274.7474\nEpoch: 2 Average validation loss: 284.1609\nValidation loss decreased. Saving model.\nTrain Epoch: 3 [0/560 (0%)]\tLoss: 239.561218\nTrain Epoch: 3 [320/560 (56%)]\tLoss: 223.099274\n====> Epoch: 3 Average loss: 226.7657\nEpoch: 3 Average validation loss: 223.9309\nValidation loss decreased. Saving model.\nTrain Epoch: 4 [0/560 (0%)]\tLoss: 207.577484\nTrain Epoch: 4 [320/560 (56%)]\tLoss: 211.463058\n====> Epoch: 4 Average loss: 188.8889\nEpoch: 4 Average validation loss: 192.5239\nValidation loss decreased. Saving model.\nTrain Epoch: 5 [0/560 (0%)]\tLoss: 171.179459\nTrain Epoch: 5 [320/560 (56%)]\tLoss: 169.162476\n====> Epoch: 5 Average loss: 169.3959\nEpoch: 5 Average validation loss: 176.6779\nValidation loss decreased. Saving model.\nTrain Epoch: 6 [0/560 (0%)]\tLoss: 174.109390\nTrain Epoch: 6 [320/560 (56%)]\tLoss: 171.589371\n====> Epoch: 6 Average loss: 161.9433\nEpoch: 6 Average validation loss: 172.9578\nValidation loss decreased. Saving model.\nTrain Epoch: 7 [0/560 (0%)]\tLoss: 180.476196\nTrain Epoch: 7 [320/560 (56%)]\tLoss: 140.780365\n====> Epoch: 7 Average loss: 158.5534\nEpoch: 7 Average validation loss: 168.4836\nValidation loss decreased. Saving model.\nTrain Epoch: 8 [0/560 (0%)]\tLoss: 150.029358\nTrain Epoch: 8 [320/560 (56%)]\tLoss: 164.160522\n====> Epoch: 8 Average loss: 156.7319\nEpoch: 8 Average validation loss: 166.3069\nValidation loss decreased. Saving model.\nTrain Epoch: 9 [0/560 (0%)]\tLoss: 155.190918\nTrain Epoch: 9 [320/560 (56%)]\tLoss: 164.989548\n====> Epoch: 9 Average loss: 154.0878\nEpoch: 9 Average validation loss: 165.8580\nValidation loss decreased. Saving model.\nTrain Epoch: 10 [0/560 (0%)]\tLoss: 129.633667\nTrain Epoch: 10 [320/560 (56%)]\tLoss: 154.477524\n====> Epoch: 10 Average loss: 152.5114\nEpoch: 10 Average validation loss: 165.1493\nValidation loss decreased. Saving model.\nTrain Epoch: 11 [0/560 (0%)]\tLoss: 160.116287\nTrain Epoch: 11 [320/560 (56%)]\tLoss: 140.832382\n====> Epoch: 11 Average loss: 150.1378\nEpoch: 11 Average validation loss: 158.3549\nValidation loss decreased. Saving model.\nTrain Epoch: 12 [0/560 (0%)]\tLoss: 140.352219\nTrain Epoch: 12 [320/560 (56%)]\tLoss: 117.703110\n====> Epoch: 12 Average loss: 141.7255\nEpoch: 12 Average validation loss: 152.1084\nValidation loss decreased. Saving model.\nTrain Epoch: 13 [0/560 (0%)]\tLoss: 138.285248\nTrain Epoch: 13 [320/560 (56%)]\tLoss: 135.898041\n====> Epoch: 13 Average loss: 137.2074\nEpoch: 13 Average validation loss: 136.4663\nValidation loss decreased. Saving model.\nTrain Epoch: 14 [0/560 (0%)]\tLoss: 129.592361\nTrain Epoch: 14 [320/560 (56%)]\tLoss: 123.681366\n====> Epoch: 14 Average loss: 130.4002\nEpoch: 14 Average validation loss: 147.5012\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 15 [0/560 (0%)]\tLoss: 122.991333\nTrain Epoch: 15 [320/560 (56%)]\tLoss: 129.325348\n====> Epoch: 15 Average loss: 126.8830\nEpoch: 15 Average validation loss: 134.8594\nValidation loss decreased. Saving model.\nTrain Epoch: 16 [0/560 (0%)]\tLoss: 110.746552\nTrain Epoch: 16 [320/560 (56%)]\tLoss: 142.075699\n====> Epoch: 16 Average loss: 119.7423\nEpoch: 16 Average validation loss: 132.1918\nValidation loss decreased. Saving model.\nTrain Epoch: 17 [0/560 (0%)]\tLoss: 138.746109\nTrain Epoch: 17 [320/560 (56%)]\tLoss: 112.218658\n====> Epoch: 17 Average loss: 119.2605\nEpoch: 17 Average validation loss: 138.7710\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 18 [0/560 (0%)]\tLoss: 105.006897\nTrain Epoch: 18 [320/560 (56%)]\tLoss: 107.439331\n====> Epoch: 18 Average loss: 115.7695\nEpoch: 18 Average validation loss: 128.3786\nValidation loss decreased. Saving model.\nTrain Epoch: 19 [0/560 (0%)]\tLoss: 126.523483\nTrain Epoch: 19 [320/560 (56%)]\tLoss: 115.383965\n====> Epoch: 19 Average loss: 114.2589\nEpoch: 19 Average validation loss: 123.1674\nValidation loss decreased. Saving model.\nTrain Epoch: 20 [0/560 (0%)]\tLoss: 125.713287\nTrain Epoch: 20 [320/560 (56%)]\tLoss: 118.533577\n====> Epoch: 20 Average loss: 113.0995\nEpoch: 20 Average validation loss: 122.9296\nValidation loss decreased. Saving model.\nTrain Epoch: 21 [0/560 (0%)]\tLoss: 104.893265\nTrain Epoch: 21 [320/560 (56%)]\tLoss: 115.047096\n====> Epoch: 21 Average loss: 113.5520\nEpoch: 21 Average validation loss: 119.8764\nValidation loss decreased. Saving model.\nTrain Epoch: 22 [0/560 (0%)]\tLoss: 104.366325\nTrain Epoch: 22 [320/560 (56%)]\tLoss: 106.707802\n====> Epoch: 22 Average loss: 110.9479\nEpoch: 22 Average validation loss: 120.2294\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 23 [0/560 (0%)]\tLoss: 116.252167\nTrain Epoch: 23 [320/560 (56%)]\tLoss: 110.664192\n====> Epoch: 23 Average loss: 110.7326\nEpoch: 23 Average validation loss: 118.5200\nValidation loss decreased. Saving model.\nTrain Epoch: 24 [0/560 (0%)]\tLoss: 115.998871\nTrain Epoch: 24 [320/560 (56%)]\tLoss: 122.752335\n====> Epoch: 24 Average loss: 110.5028\nEpoch: 24 Average validation loss: 121.7710\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 25 [0/560 (0%)]\tLoss: 89.832008\nTrain Epoch: 25 [320/560 (56%)]\tLoss: 108.833755\n====> Epoch: 25 Average loss: 108.6251\nEpoch: 25 Average validation loss: 120.1146\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 26 [0/560 (0%)]\tLoss: 104.029335\nTrain Epoch: 26 [320/560 (56%)]\tLoss: 105.063728\n====> Epoch: 26 Average loss: 107.7772\nEpoch: 26 Average validation loss: 127.1689\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 27 [0/560 (0%)]\tLoss: 101.623306\nTrain Epoch: 27 [320/560 (56%)]\tLoss: 98.420654\n====> Epoch: 27 Average loss: 108.0217\nEpoch: 27 Average validation loss: 114.7003\nValidation loss decreased. Saving model.\nTrain Epoch: 28 [0/560 (0%)]\tLoss: 105.868683\nTrain Epoch: 28 [320/560 (56%)]\tLoss: 104.613426\n====> Epoch: 28 Average loss: 106.0178\nEpoch: 28 Average validation loss: 114.4755\nValidation loss decreased. Saving model.\nTrain Epoch: 29 [0/560 (0%)]\tLoss: 102.076363\nTrain Epoch: 29 [320/560 (56%)]\tLoss: 116.121887\n====> Epoch: 29 Average loss: 106.1232\nEpoch: 29 Average validation loss: 115.6079\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 30 [0/560 (0%)]\tLoss: 107.016769\nTrain Epoch: 30 [320/560 (56%)]\tLoss: 102.956917\n====> Epoch: 30 Average loss: 107.3622\nEpoch: 30 Average validation loss: 118.9288\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 31 [0/560 (0%)]\tLoss: 89.804527\nTrain Epoch: 31 [320/560 (56%)]\tLoss: 105.931366\n====> Epoch: 31 Average loss: 105.8200\nEpoch: 31 Average validation loss: 117.1915\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 32 [0/560 (0%)]\tLoss: 118.540253\nTrain Epoch: 32 [320/560 (56%)]\tLoss: 105.318985\n====> Epoch: 32 Average loss: 106.8774\nEpoch: 32 Average validation loss: 112.4450\nValidation loss decreased. Saving model.\nTrain Epoch: 33 [0/560 (0%)]\tLoss: 92.155075\nTrain Epoch: 33 [320/560 (56%)]\tLoss: 87.857147\n====> Epoch: 33 Average loss: 103.8955\nEpoch: 33 Average validation loss: 112.3045\nValidation loss decreased. Saving model.\nTrain Epoch: 34 [0/560 (0%)]\tLoss: 104.455620\nTrain Epoch: 34 [320/560 (56%)]\tLoss: 96.692474\n====> Epoch: 34 Average loss: 103.9962\nEpoch: 34 Average validation loss: 114.6708\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 35 [0/560 (0%)]\tLoss: 93.211014\nTrain Epoch: 35 [320/560 (56%)]\tLoss: 89.263107\n====> Epoch: 35 Average loss: 103.2018\nEpoch: 35 Average validation loss: 116.2210\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 36 [0/560 (0%)]\tLoss: 98.640587\nTrain Epoch: 36 [320/560 (56%)]\tLoss: 94.660606\n====> Epoch: 36 Average loss: 103.0938\nEpoch: 36 Average validation loss: 115.3918\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 37 [0/560 (0%)]\tLoss: 98.424431\nTrain Epoch: 37 [320/560 (56%)]\tLoss: 105.633118\n====> Epoch: 37 Average loss: 103.0625\nEpoch: 37 Average validation loss: 117.7537\nValidation loss did not improve. Patience: 4/10\nTrain Epoch: 38 [0/560 (0%)]\tLoss: 94.548714\nTrain Epoch: 38 [320/560 (56%)]\tLoss: 97.683411\n====> Epoch: 38 Average loss: 101.8773\nEpoch: 38 Average validation loss: 122.1622\nValidation loss did not improve. Patience: 5/10\nTrain Epoch: 39 [0/560 (0%)]\tLoss: 92.264633\nTrain Epoch: 39 [320/560 (56%)]\tLoss: 101.141594\n====> Epoch: 39 Average loss: 100.5428\nEpoch: 39 Average validation loss: 116.0329\nValidation loss did not improve. Patience: 6/10\nTrain Epoch: 40 [0/560 (0%)]\tLoss: 102.013451\nTrain Epoch: 40 [320/560 (56%)]\tLoss: 94.097771\n====> Epoch: 40 Average loss: 99.2863\nEpoch: 40 Average validation loss: 107.3879\nValidation loss decreased. Saving model.\nTrain Epoch: 41 [0/560 (0%)]\tLoss: 102.997574\nTrain Epoch: 41 [320/560 (56%)]\tLoss: 88.362747\n====> Epoch: 41 Average loss: 99.4937\nEpoch: 41 Average validation loss: 119.1941\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 42 [0/560 (0%)]\tLoss: 99.910149\nTrain Epoch: 42 [320/560 (56%)]\tLoss: 87.714935\n====> Epoch: 42 Average loss: 99.4505\nEpoch: 42 Average validation loss: 112.6558\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 43 [0/560 (0%)]\tLoss: 113.021698\nTrain Epoch: 43 [320/560 (56%)]\tLoss: 90.906403\n====> Epoch: 43 Average loss: 99.4398\nEpoch: 43 Average validation loss: 108.8898\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 44 [0/560 (0%)]\tLoss: 81.064621\nTrain Epoch: 44 [320/560 (56%)]\tLoss: 93.307785\n====> Epoch: 44 Average loss: 98.4480\nEpoch: 44 Average validation loss: 110.1221\nValidation loss did not improve. Patience: 4/10\nTrain Epoch: 45 [0/560 (0%)]\tLoss: 101.024689\nTrain Epoch: 45 [320/560 (56%)]\tLoss: 84.464722\n====> Epoch: 45 Average loss: 97.9969\nEpoch: 45 Average validation loss: 107.6720\nValidation loss did not improve. Patience: 5/10\nTrain Epoch: 46 [0/560 (0%)]\tLoss: 107.614349\nTrain Epoch: 46 [320/560 (56%)]\tLoss: 100.460114\n====> Epoch: 46 Average loss: 97.7321\nEpoch: 46 Average validation loss: 114.1105\nValidation loss did not improve. Patience: 6/10\nTrain Epoch: 47 [0/560 (0%)]\tLoss: 92.932213\nTrain Epoch: 47 [320/560 (56%)]\tLoss: 85.357803\n====> Epoch: 47 Average loss: 97.2851\nEpoch: 47 Average validation loss: 108.3899\nValidation loss did not improve. Patience: 7/10\nTrain Epoch: 48 [0/560 (0%)]\tLoss: 96.935509\nTrain Epoch: 48 [320/560 (56%)]\tLoss: 86.722954\n====> Epoch: 48 Average loss: 95.3231\nEpoch: 48 Average validation loss: 107.3522\nValidation loss decreased. Saving model.\nTrain Epoch: 49 [0/560 (0%)]\tLoss: 95.113853\nTrain Epoch: 49 [320/560 (56%)]\tLoss: 99.304352\n====> Epoch: 49 Average loss: 96.2116\nEpoch: 49 Average validation loss: 104.4197\nValidation loss decreased. Saving model.\nTrain Epoch: 50 [0/560 (0%)]\tLoss: 84.047302\nTrain Epoch: 50 [320/560 (56%)]\tLoss: 88.064102\n====> Epoch: 50 Average loss: 96.0961\nEpoch: 50 Average validation loss: 103.2447\nValidation loss decreased. Saving model.\nTrain Epoch: 51 [0/560 (0%)]\tLoss: 85.693062\nTrain Epoch: 51 [320/560 (56%)]\tLoss: 88.190933\n====> Epoch: 51 Average loss: 94.4244\nEpoch: 51 Average validation loss: 103.1812\nValidation loss decreased. Saving model.\nTrain Epoch: 52 [0/560 (0%)]\tLoss: 82.359253\nTrain Epoch: 52 [320/560 (56%)]\tLoss: 88.265755\n====> Epoch: 52 Average loss: 95.0448\nEpoch: 52 Average validation loss: 104.6764\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 53 [0/560 (0%)]\tLoss: 97.426147\nTrain Epoch: 53 [320/560 (56%)]\tLoss: 95.637421\n====> Epoch: 53 Average loss: 93.3773\nEpoch: 53 Average validation loss: 112.6157\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 54 [0/560 (0%)]\tLoss: 105.462166\nTrain Epoch: 54 [320/560 (56%)]\tLoss: 91.497818\n====> Epoch: 54 Average loss: 95.4409\nEpoch: 54 Average validation loss: 103.7936\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 55 [0/560 (0%)]\tLoss: 92.575867\nTrain Epoch: 55 [320/560 (56%)]\tLoss: 86.262581\n====> Epoch: 55 Average loss: 93.5843\nEpoch: 55 Average validation loss: 104.3444\nValidation loss did not improve. Patience: 4/10\nTrain Epoch: 56 [0/560 (0%)]\tLoss: 99.793633\nTrain Epoch: 56 [320/560 (56%)]\tLoss: 91.055267\n====> Epoch: 56 Average loss: 93.2274\nEpoch: 56 Average validation loss: 103.8650\nValidation loss did not improve. Patience: 5/10\nTrain Epoch: 57 [0/560 (0%)]\tLoss: 99.873413\nTrain Epoch: 57 [320/560 (56%)]\tLoss: 105.961800\n====> Epoch: 57 Average loss: 93.6864\nEpoch: 57 Average validation loss: 112.5037\nValidation loss did not improve. Patience: 6/10\nTrain Epoch: 58 [0/560 (0%)]\tLoss: 99.776260\nTrain Epoch: 58 [320/560 (56%)]\tLoss: 92.790337\n====> Epoch: 58 Average loss: 92.5711\nEpoch: 58 Average validation loss: 100.3473\nValidation loss decreased. Saving model.\nTrain Epoch: 59 [0/560 (0%)]\tLoss: 87.694336\nTrain Epoch: 59 [320/560 (56%)]\tLoss: 91.172775\n====> Epoch: 59 Average loss: 90.7112\nEpoch: 59 Average validation loss: 101.6057\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 60 [0/560 (0%)]\tLoss: 101.914291\nTrain Epoch: 60 [320/560 (56%)]\tLoss: 113.351997\n====> Epoch: 60 Average loss: 91.7073\nEpoch: 60 Average validation loss: 98.9993\nValidation loss decreased. Saving model.\nTrain Epoch: 61 [0/560 (0%)]\tLoss: 90.006966\nTrain Epoch: 61 [320/560 (56%)]\tLoss: 91.721336\n====> Epoch: 61 Average loss: 90.6382\nEpoch: 61 Average validation loss: 100.9511\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 62 [0/560 (0%)]\tLoss: 87.860550\nTrain Epoch: 62 [320/560 (56%)]\tLoss: 102.634537\n====> Epoch: 62 Average loss: 92.1388\nEpoch: 62 Average validation loss: 105.1032\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 63 [0/560 (0%)]\tLoss: 82.682266\nTrain Epoch: 63 [320/560 (56%)]\tLoss: 99.454041\n====> Epoch: 63 Average loss: 90.7278\nEpoch: 63 Average validation loss: 110.1833\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 64 [0/560 (0%)]\tLoss: 97.173477\nTrain Epoch: 64 [320/560 (56%)]\tLoss: 82.910019\n====> Epoch: 64 Average loss: 90.3759\nEpoch: 64 Average validation loss: 100.5898\nValidation loss did not improve. Patience: 4/10\nTrain Epoch: 65 [0/560 (0%)]\tLoss: 88.628502\nTrain Epoch: 65 [320/560 (56%)]\tLoss: 74.327133\n====> Epoch: 65 Average loss: 92.1097\nEpoch: 65 Average validation loss: 106.2662\nValidation loss did not improve. Patience: 5/10\nTrain Epoch: 66 [0/560 (0%)]\tLoss: 92.143692\nTrain Epoch: 66 [320/560 (56%)]\tLoss: 84.601723\n====> Epoch: 66 Average loss: 90.8834\nEpoch: 66 Average validation loss: 100.3091\nValidation loss did not improve. Patience: 6/10\nTrain Epoch: 67 [0/560 (0%)]\tLoss: 72.384598\nTrain Epoch: 67 [320/560 (56%)]\tLoss: 87.106178\n====> Epoch: 67 Average loss: 90.2258\nEpoch: 67 Average validation loss: 103.3108\nValidation loss did not improve. Patience: 7/10\nTrain Epoch: 68 [0/560 (0%)]\tLoss: 76.863106\nTrain Epoch: 68 [320/560 (56%)]\tLoss: 103.441559\n====> Epoch: 68 Average loss: 91.6103\nEpoch: 68 Average validation loss: 98.3929\nValidation loss decreased. Saving model.\nTrain Epoch: 69 [0/560 (0%)]\tLoss: 79.245705\nTrain Epoch: 69 [320/560 (56%)]\tLoss: 81.451309\n====> Epoch: 69 Average loss: 89.0469\nEpoch: 69 Average validation loss: 97.0268\nValidation loss decreased. Saving model.\nTrain Epoch: 70 [0/560 (0%)]\tLoss: 93.251892\nTrain Epoch: 70 [320/560 (56%)]\tLoss: 81.117020\n====> Epoch: 70 Average loss: 87.3182\nEpoch: 70 Average validation loss: 98.3064\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 71 [0/560 (0%)]\tLoss: 91.835358\nTrain Epoch: 71 [320/560 (56%)]\tLoss: 90.137993\n====> Epoch: 71 Average loss: 86.8297\nEpoch: 71 Average validation loss: 97.4537\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 72 [0/560 (0%)]\tLoss: 78.035049\nTrain Epoch: 72 [320/560 (56%)]\tLoss: 92.459885\n====> Epoch: 72 Average loss: 88.2897\nEpoch: 72 Average validation loss: 98.8795\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 73 [0/560 (0%)]\tLoss: 94.443527\nTrain Epoch: 73 [320/560 (56%)]\tLoss: 85.953026\n====> Epoch: 73 Average loss: 86.6093\nEpoch: 73 Average validation loss: 100.8918\nValidation loss did not improve. Patience: 4/10\nTrain Epoch: 74 [0/560 (0%)]\tLoss: 76.593231\nTrain Epoch: 74 [320/560 (56%)]\tLoss: 77.600632\n====> Epoch: 74 Average loss: 86.0336\nEpoch: 74 Average validation loss: 98.6921\nValidation loss did not improve. Patience: 5/10\nTrain Epoch: 75 [0/560 (0%)]\tLoss: 72.571548\nTrain Epoch: 75 [320/560 (56%)]\tLoss: 80.159012\n====> Epoch: 75 Average loss: 86.3974\nEpoch: 75 Average validation loss: 101.3190\nValidation loss did not improve. Patience: 6/10\nTrain Epoch: 76 [0/560 (0%)]\tLoss: 91.480568\nTrain Epoch: 76 [320/560 (56%)]\tLoss: 75.532021\n====> Epoch: 76 Average loss: 85.6771\nEpoch: 76 Average validation loss: 104.0802\nValidation loss did not improve. Patience: 7/10\nTrain Epoch: 77 [0/560 (0%)]\tLoss: 70.964111\nTrain Epoch: 77 [320/560 (56%)]\tLoss: 83.748383\n====> Epoch: 77 Average loss: 85.8656\nEpoch: 77 Average validation loss: 113.8866\nValidation loss did not improve. Patience: 8/10\nTrain Epoch: 78 [0/560 (0%)]\tLoss: 81.246796\nTrain Epoch: 78 [320/560 (56%)]\tLoss: 82.090271\n====> Epoch: 78 Average loss: 88.1527\nEpoch: 78 Average validation loss: 97.6792\nValidation loss did not improve. Patience: 9/10\nTrain Epoch: 79 [0/560 (0%)]\tLoss: 76.531479\nTrain Epoch: 79 [320/560 (56%)]\tLoss: 82.750748\n====> Epoch: 79 Average loss: 86.0088\nEpoch: 79 Average validation loss: 96.5665\nValidation loss decreased. Saving model.\nTrain Epoch: 80 [0/560 (0%)]\tLoss: 76.973480\nTrain Epoch: 80 [320/560 (56%)]\tLoss: 91.255150\n====> Epoch: 80 Average loss: 86.5152\nEpoch: 80 Average validation loss: 98.2808\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 81 [0/560 (0%)]\tLoss: 85.884712\nTrain Epoch: 81 [320/560 (56%)]\tLoss: 80.667328\n====> Epoch: 81 Average loss: 84.6387\nEpoch: 81 Average validation loss: 103.5061\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 82 [0/560 (0%)]\tLoss: 59.308987\nTrain Epoch: 82 [320/560 (56%)]\tLoss: 88.313606\n====> Epoch: 82 Average loss: 84.2062\nEpoch: 82 Average validation loss: 100.9676\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 83 [0/560 (0%)]\tLoss: 77.837410\nTrain Epoch: 83 [320/560 (56%)]\tLoss: 77.329361\n====> Epoch: 83 Average loss: 86.2858\nEpoch: 83 Average validation loss: 100.5275\nValidation loss did not improve. Patience: 4/10\nTrain Epoch: 84 [0/560 (0%)]\tLoss: 85.621628\nTrain Epoch: 84 [320/560 (56%)]\tLoss: 73.495407\n====> Epoch: 84 Average loss: 84.0545\nEpoch: 84 Average validation loss: 97.3628\nValidation loss did not improve. Patience: 5/10\nTrain Epoch: 85 [0/560 (0%)]\tLoss: 94.900856\nTrain Epoch: 85 [320/560 (56%)]\tLoss: 81.431488\n====> Epoch: 85 Average loss: 83.2459\nEpoch: 85 Average validation loss: 93.0783\nValidation loss decreased. Saving model.\nTrain Epoch: 86 [0/560 (0%)]\tLoss: 84.672379\nTrain Epoch: 86 [320/560 (56%)]\tLoss: 83.477112\n====> Epoch: 86 Average loss: 84.1214\nEpoch: 86 Average validation loss: 101.7454\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 87 [0/560 (0%)]\tLoss: 84.315445\nTrain Epoch: 87 [320/560 (56%)]\tLoss: 78.235985\n====> Epoch: 87 Average loss: 83.4299\nEpoch: 87 Average validation loss: 101.3303\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 88 [0/560 (0%)]\tLoss: 86.878052\nTrain Epoch: 88 [320/560 (56%)]\tLoss: 77.831451\n====> Epoch: 88 Average loss: 84.0418\nEpoch: 88 Average validation loss: 96.3070\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 89 [0/560 (0%)]\tLoss: 78.453720\nTrain Epoch: 89 [320/560 (56%)]\tLoss: 83.446487\n====> Epoch: 89 Average loss: 83.5995\nEpoch: 89 Average validation loss: 94.0651\nValidation loss did not improve. Patience: 4/10\nTrain Epoch: 90 [0/560 (0%)]\tLoss: 71.122093\nTrain Epoch: 90 [320/560 (56%)]\tLoss: 88.510666\n====> Epoch: 90 Average loss: 81.9595\nEpoch: 90 Average validation loss: 95.6987\nValidation loss did not improve. Patience: 5/10\nTrain Epoch: 91 [0/560 (0%)]\tLoss: 89.215508\nTrain Epoch: 91 [320/560 (56%)]\tLoss: 77.841446\n====> Epoch: 91 Average loss: 82.4938\nEpoch: 91 Average validation loss: 97.2967\nValidation loss did not improve. Patience: 6/10\nTrain Epoch: 92 [0/560 (0%)]\tLoss: 89.128021\nTrain Epoch: 92 [320/560 (56%)]\tLoss: 85.616638\n====> Epoch: 92 Average loss: 84.1085\nEpoch: 92 Average validation loss: 99.6838\nValidation loss did not improve. Patience: 7/10\nTrain Epoch: 93 [0/560 (0%)]\tLoss: 74.823982\nTrain Epoch: 93 [320/560 (56%)]\tLoss: 90.767349\n====> Epoch: 93 Average loss: 82.4782\nEpoch: 93 Average validation loss: 97.3272\nValidation loss did not improve. Patience: 8/10\nTrain Epoch: 94 [0/560 (0%)]\tLoss: 85.740501\nTrain Epoch: 94 [320/560 (56%)]\tLoss: 81.354599\n====> Epoch: 94 Average loss: 82.8713\nEpoch: 94 Average validation loss: 97.7568\nValidation loss did not improve. Patience: 9/10\nTrain Epoch: 95 [0/560 (0%)]\tLoss: 103.232407\nTrain Epoch: 95 [320/560 (56%)]\tLoss: 92.362976\n====> Epoch: 95 Average loss: 82.6208\nEpoch: 95 Average validation loss: 95.8310\nValidation loss did not improve. Patience: 10/10\nEarly stopping triggered!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from torchvision.utils import save_image\n\ndef test(epoch):\n    model.eval()\n    with torch.no_grad():\n        for i, data in enumerate(train_loader):\n            data = data.to(device)\n            recon_batch, mu, logvar = model(data)\n\n            if i == 0:\n                n = min(data.size(0), 8)   # take up to 8 samples\n\n                # concat original and reconstructions along the batch dimension\n                comparison = torch.cat([data[:n],\n                                        recon_batch[:n]])\n\n                save_image(comparison.cpu(),\n                           f\"/kaggle/working/reconstruction_{epoch}.png\",\n                           nrow=n)\n            break   # only first batch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:03:24.553689Z","iopub.execute_input":"2025-09-07T17:03:24.553947Z","iopub.status.idle":"2025-09-07T17:03:24.559348Z","shell.execute_reply.started":"2025-09-07T17:03:24.553926Z","shell.execute_reply":"2025-09-07T17:03:24.558663Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"test(95)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:03:32.197232Z","iopub.execute_input":"2025-09-07T17:03:32.197911Z","iopub.status.idle":"2025-09-07T17:03:33.522311Z","shell.execute_reply.started":"2025-09-07T17:03:32.197887Z","shell.execute_reply":"2025-09-07T17:03:33.521338Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"**Results: Check Img2 on GITHUB**","metadata":{}},{"cell_type":"markdown","source":"**Comments:**\nThe images show that there is definitely an improvement in the generation but it is not very good","metadata":{}},{"cell_type":"markdown","source":"**Experiment:** Trying to increase the complexity of the model","metadata":{}},{"cell_type":"code","source":"class VAE2(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        ## Encoder\n        self.conv1 = nn.Conv2d(3, 32, 7, stride=2, padding=3) \n        \n        self.res1 = ResidualBlock(32, 64, 2)    \n        self.res2 = ResidualBlock(64, 128, 2)   \n        self.res3 = ResidualBlock(128, 256, 2)  \n        self.res4 = ResidualBlock(256, 512, 2)  \n        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        \n        self.fc_mean = nn.Linear(512, 128)\n        self.fc_var = nn.Linear(512, 128)\n\n        ## Decoder\n        self.fc1 = nn.Linear(128, 512 * 7 * 7) \n\n        self.up1 = nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1)\n        self.bn1 = nn.BatchNorm2d(256)\n        self.up2 = nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.up3 = nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1)\n        self.bn3 = nn.BatchNorm2d(64)\n        self.up4 = nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1)\n        self.bn4 = nn.BatchNorm2d(32)\n        self.up5 = nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1)\n\n    def encode(self,x):\n        x = F.relu(self.conv1(x))\n        x = self.res1(x)\n        x = self.res2(x)\n        x = self.res3(x)\n        x = self.res4(x)\n        x = torch.flatten(self.avgpool(x),1)\n        mean = self.fc_mean(x)\n        log_var = self.fc_var(x)\n        return mean,log_var\n\n    def reparameterize(self,mu,log_var):\n        std = torch.exp(0.5*log_var) #0.5 prevents very huge value \n        eps = torch.randn_like(std)\n        z = mu + std*eps\n        return z\n\n    def decode(self,z):\n        out = F.relu(self.fc1(z))\n        out = out.view(-1,512,7,7)\n        out = F.relu(self.bn1(self.up1(out)))\n        out = F.relu(self.bn2(self.up2(out)))\n        out = F.relu(self.bn3(self.up3(out)))\n        out = F.relu(self.bn4(self.up4(out)))\n        out = torch.sigmoid(self.up5(out))\n        return out\n\n    def loss_function(self,out,x,mu,log_vars,beta=0.5):\n        loss = nn.MSELoss(reduction='mean')\n        recon_loss = loss(x,out)*(x.shape[1] * x.shape[2] * x.shape[3])\n        \n        kld = -0.5 * torch.sum(1 + log_vars - mu.pow(2) - log_vars.exp())\n\n        return recon_loss+(kld*beta)\n\n    def forward(self,x):\n        mu,log_vars = self.encode(x)\n        z = self.reparameterize(mu,log_vars)\n        out = self.decode(z)\n        return out,mu,log_vars","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:40:44.005192Z","iopub.execute_input":"2025-09-07T17:40:44.005865Z","iopub.status.idle":"2025-09-07T17:40:44.018596Z","shell.execute_reply.started":"2025-09-07T17:40:44.005837Z","shell.execute_reply":"2025-09-07T17:40:44.017905Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from torch import optim\ndevice = torch.accelerator.current_accelerator() if torch.accelerator.is_available() else 'cpu'\nmodel2 = VAE2().to(device)\nmodel2.load_state_dict(torch.load(\"/kaggle/input/variationalautoencoderunderwaterimages/pytorch/default/1/best_model2.pth\"))\noptimizer = optim.Adam(model2.parameters(), lr=1e-3)\nlog_interval = 10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:44:41.425489Z","iopub.execute_input":"2025-09-07T17:44:41.425732Z","iopub.status.idle":"2025-09-07T17:44:41.600513Z","shell.execute_reply.started":"2025-09-07T17:44:41.425717Z","shell.execute_reply":"2025-09-07T17:44:41.599851Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def train2(epoch):\n    model2.train() #what this does\n    train_loss = 0\n    for batch_idx, data in enumerate(train_loader):\n        data = data.to(device)\n        optimizer.zero_grad()\n        recon_batch, mu, logvar = model2(data)\n        loss = model2.loss_function(recon_batch, data, mu, logvar)\n        loss.backward()\n        train_loss += loss.item()\n        optimizer.step()\n        if batch_idx % log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader),\n                loss.item() / len(data)))\n\n    print('====> Epoch: {} Average loss: {:.4f}'.format(\n          epoch, train_loss / len(train_loader.dataset)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:44:45.988600Z","iopub.execute_input":"2025-09-07T17:44:45.988862Z","iopub.status.idle":"2025-09-07T17:44:45.994191Z","shell.execute_reply.started":"2025-09-07T17:44:45.988840Z","shell.execute_reply":"2025-09-07T17:44:45.993504Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"epochs = 100\nbest_val_loss = float('inf')\npatience = 10 \npatience_counter = 0\n\n\nfor epoch in range(1, epochs + 1):\n    train2(epoch)\n    \n    \n    model2.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for data in val_loader:\n            data = data.to(device)\n            recon_batch, mu, logvar = model2(data)\n            loss = model2.loss_function(recon_batch, data, mu, logvar)\n            val_loss += loss.item()\n    \n    avg_val_loss = val_loss / len(val_loader.dataset)\n    print(f'Epoch: {epoch} Average validation loss: {avg_val_loss:.4f}')\n\n    \n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        torch.save(model2.state_dict(), 'best_model2.pth')\n        print('Validation loss decreased. Saving model.')\n    else:\n        patience_counter += 1\n        print(f'Validation loss did not improve. Patience: {patience_counter}/{patience}')\n\n    if patience_counter >= patience:\n        print('Early stopping triggered!')\n        break ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:14:28.085105Z","iopub.execute_input":"2025-09-07T17:14:28.085351Z","execution_failed":"2025-09-07T17:38:31.737Z"}},"outputs":[{"name":"stdout","text":"Train Epoch: 1 [0/560 (0%)]\tLoss: 342.077393\nTrain Epoch: 1 [320/560 (56%)]\tLoss: 217.246063\n====> Epoch: 1 Average loss: 238.8863\nEpoch: 1 Average validation loss: 285.3499\nValidation loss decreased. Saving model.\nTrain Epoch: 2 [0/560 (0%)]\tLoss: 185.909058\nTrain Epoch: 2 [320/560 (56%)]\tLoss: 158.440552\n====> Epoch: 2 Average loss: 184.6625\nEpoch: 2 Average validation loss: 195.3419\nValidation loss decreased. Saving model.\nTrain Epoch: 3 [0/560 (0%)]\tLoss: 167.989929\nTrain Epoch: 3 [320/560 (56%)]\tLoss: 185.367767\n====> Epoch: 3 Average loss: 178.0129\nEpoch: 3 Average validation loss: 181.7257\nValidation loss decreased. Saving model.\nTrain Epoch: 4 [0/560 (0%)]\tLoss: 166.059967\nTrain Epoch: 4 [320/560 (56%)]\tLoss: 143.300201\n====> Epoch: 4 Average loss: 172.1455\nEpoch: 4 Average validation loss: 178.3906\nValidation loss decreased. Saving model.\nTrain Epoch: 5 [0/560 (0%)]\tLoss: 178.931519\nTrain Epoch: 5 [320/560 (56%)]\tLoss: 157.996948\n====> Epoch: 5 Average loss: 166.7676\nEpoch: 5 Average validation loss: 177.2870\nValidation loss decreased. Saving model.\nTrain Epoch: 6 [0/560 (0%)]\tLoss: 158.628632\nTrain Epoch: 6 [320/560 (56%)]\tLoss: 154.853363\n====> Epoch: 6 Average loss: 157.9762\nEpoch: 6 Average validation loss: 169.9976\nValidation loss decreased. Saving model.\nTrain Epoch: 7 [0/560 (0%)]\tLoss: 123.104164\nTrain Epoch: 7 [320/560 (56%)]\tLoss: 175.997971\n====> Epoch: 7 Average loss: 153.7622\nEpoch: 7 Average validation loss: 200.9513\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 8 [0/560 (0%)]\tLoss: 151.524353\nTrain Epoch: 8 [320/560 (56%)]\tLoss: 182.793121\n====> Epoch: 8 Average loss: 151.3638\nEpoch: 8 Average validation loss: 183.7894\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 9 [0/560 (0%)]\tLoss: 153.793732\nTrain Epoch: 9 [320/560 (56%)]\tLoss: 140.963501\n====> Epoch: 9 Average loss: 147.8153\nEpoch: 9 Average validation loss: 173.2484\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 10 [0/560 (0%)]\tLoss: 150.630081\nTrain Epoch: 10 [320/560 (56%)]\tLoss: 136.122086\n====> Epoch: 10 Average loss: 153.4495\nEpoch: 10 Average validation loss: 167.1372\nValidation loss decreased. Saving model.\nTrain Epoch: 11 [0/560 (0%)]\tLoss: 157.980072\nTrain Epoch: 11 [320/560 (56%)]\tLoss: 149.164474\n====> Epoch: 11 Average loss: 151.6274\nEpoch: 11 Average validation loss: 155.0328\nValidation loss decreased. Saving model.\nTrain Epoch: 12 [0/560 (0%)]\tLoss: 177.498413\nTrain Epoch: 12 [320/560 (56%)]\tLoss: 164.615402\n====> Epoch: 12 Average loss: 148.4339\nEpoch: 12 Average validation loss: 162.3231\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 13 [0/560 (0%)]\tLoss: 130.197845\nTrain Epoch: 13 [320/560 (56%)]\tLoss: 143.228485\n====> Epoch: 13 Average loss: 145.7902\nEpoch: 13 Average validation loss: 158.3967\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 14 [0/560 (0%)]\tLoss: 125.375412\nTrain Epoch: 14 [320/560 (56%)]\tLoss: 119.547035\n====> Epoch: 14 Average loss: 142.6615\nEpoch: 14 Average validation loss: 153.7456\nValidation loss decreased. Saving model.\nTrain Epoch: 15 [0/560 (0%)]\tLoss: 132.244598\nTrain Epoch: 15 [320/560 (56%)]\tLoss: 150.239395\n====> Epoch: 15 Average loss: 141.0806\nEpoch: 15 Average validation loss: 151.7929\nValidation loss decreased. Saving model.\nTrain Epoch: 16 [0/560 (0%)]\tLoss: 135.362915\nTrain Epoch: 16 [320/560 (56%)]\tLoss: 111.522095\n====> Epoch: 16 Average loss: 136.6716\nEpoch: 16 Average validation loss: 139.5378\nValidation loss decreased. Saving model.\nTrain Epoch: 17 [0/560 (0%)]\tLoss: 138.974762\nTrain Epoch: 17 [320/560 (56%)]\tLoss: 144.450134\n====> Epoch: 17 Average loss: 130.6626\nEpoch: 17 Average validation loss: 141.2513\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 18 [0/560 (0%)]\tLoss: 122.037048\nTrain Epoch: 18 [320/560 (56%)]\tLoss: 125.009033\n====> Epoch: 18 Average loss: 125.4585\nEpoch: 18 Average validation loss: 133.6720\nValidation loss decreased. Saving model.\nTrain Epoch: 19 [0/560 (0%)]\tLoss: 140.946442\nTrain Epoch: 19 [320/560 (56%)]\tLoss: 99.299423\n====> Epoch: 19 Average loss: 125.4903\nEpoch: 19 Average validation loss: 143.7606\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 20 [0/560 (0%)]\tLoss: 124.466866\nTrain Epoch: 20 [320/560 (56%)]\tLoss: 138.182663\n====> Epoch: 20 Average loss: 127.4977\nEpoch: 20 Average validation loss: 134.7227\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 21 [0/560 (0%)]\tLoss: 153.572693\nTrain Epoch: 21 [320/560 (56%)]\tLoss: 123.599617\n====> Epoch: 21 Average loss: 122.3757\nEpoch: 21 Average validation loss: 129.5651\nValidation loss decreased. Saving model.\nTrain Epoch: 22 [0/560 (0%)]\tLoss: 112.694595\nTrain Epoch: 22 [320/560 (56%)]\tLoss: 110.381790\n====> Epoch: 22 Average loss: 120.2512\nEpoch: 22 Average validation loss: 138.1363\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 23 [0/560 (0%)]\tLoss: 116.625038\nTrain Epoch: 23 [320/560 (56%)]\tLoss: 112.891571\n====> Epoch: 23 Average loss: 116.3007\nEpoch: 23 Average validation loss: 122.6206\nValidation loss decreased. Saving model.\nTrain Epoch: 24 [0/560 (0%)]\tLoss: 127.297119\nTrain Epoch: 24 [320/560 (56%)]\tLoss: 111.381035\n====> Epoch: 24 Average loss: 114.4839\nEpoch: 24 Average validation loss: 125.8669\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 25 [0/560 (0%)]\tLoss: 103.282326\nTrain Epoch: 25 [320/560 (56%)]\tLoss: 116.320404\n====> Epoch: 25 Average loss: 115.3844\nEpoch: 25 Average validation loss: 122.8965\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 26 [0/560 (0%)]\tLoss: 99.610680\nTrain Epoch: 26 [320/560 (56%)]\tLoss: 112.672409\n====> Epoch: 26 Average loss: 113.2068\nEpoch: 26 Average validation loss: 146.0587\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 27 [0/560 (0%)]\tLoss: 93.547089\nTrain Epoch: 27 [320/560 (56%)]\tLoss: 111.814560\n====> Epoch: 27 Average loss: 115.3839\nEpoch: 27 Average validation loss: 123.5450\nValidation loss did not improve. Patience: 4/10\nTrain Epoch: 28 [0/560 (0%)]\tLoss: 124.075058\nTrain Epoch: 28 [320/560 (56%)]\tLoss: 115.133606\n====> Epoch: 28 Average loss: 112.7435\nEpoch: 28 Average validation loss: 122.5935\nValidation loss decreased. Saving model.\nTrain Epoch: 29 [0/560 (0%)]\tLoss: 116.658882\nTrain Epoch: 29 [320/560 (56%)]\tLoss: 93.852386\n====> Epoch: 29 Average loss: 111.9810\nEpoch: 29 Average validation loss: 123.1650\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 30 [0/560 (0%)]\tLoss: 115.812057\nTrain Epoch: 30 [320/560 (56%)]\tLoss: 97.111786\n====> Epoch: 30 Average loss: 109.0346\nEpoch: 30 Average validation loss: 121.3140\nValidation loss decreased. Saving model.\nTrain Epoch: 31 [0/560 (0%)]\tLoss: 105.506798\nTrain Epoch: 31 [320/560 (56%)]\tLoss: 103.496284\n====> Epoch: 31 Average loss: 108.3271\nEpoch: 31 Average validation loss: 117.9576\nValidation loss decreased. Saving model.\nTrain Epoch: 32 [0/560 (0%)]\tLoss: 109.053040\nTrain Epoch: 32 [320/560 (56%)]\tLoss: 101.883041\n====> Epoch: 32 Average loss: 108.7602\nEpoch: 32 Average validation loss: 115.1927\nValidation loss decreased. Saving model.\nTrain Epoch: 33 [0/560 (0%)]\tLoss: 112.500977\nTrain Epoch: 33 [320/560 (56%)]\tLoss: 91.843887\n====> Epoch: 33 Average loss: 106.3212\nEpoch: 33 Average validation loss: 120.0405\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 34 [0/560 (0%)]\tLoss: 113.688667\nTrain Epoch: 34 [320/560 (56%)]\tLoss: 97.141045\n====> Epoch: 34 Average loss: 106.3009\nEpoch: 34 Average validation loss: 111.0515\nValidation loss decreased. Saving model.\nTrain Epoch: 35 [0/560 (0%)]\tLoss: 99.732582\nTrain Epoch: 35 [320/560 (56%)]\tLoss: 92.136902\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"epochs = 100\nbest_val_loss = float('inf')\npatience = 10 \npatience_counter = 0\n\n\nfor epoch in range(36, epochs + 1):\n    train2(epoch)\n    \n    \n    model2.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for data in val_loader:\n            data = data.to(device)\n            recon_batch, mu, logvar = model2(data)\n            loss = model2.loss_function(recon_batch, data, mu, logvar)\n            val_loss += loss.item()\n    \n    avg_val_loss = val_loss / len(val_loader.dataset)\n    print(f'Epoch: {epoch} Average validation loss: {avg_val_loss:.4f}')\n\n    \n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        torch.save(model2.state_dict(), 'best_model2.pth')\n        print('Validation loss decreased. Saving model.')\n    else:\n        patience_counter += 1\n        print(f'Validation loss did not improve. Patience: {patience_counter}/{patience}')\n\n    if patience_counter >= patience:\n        print('Early stopping triggered!')\n        break ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:46:44.352714Z","iopub.execute_input":"2025-09-07T17:46:44.353234Z","iopub.status.idle":"2025-09-07T18:07:03.475665Z","shell.execute_reply.started":"2025-09-07T17:46:44.353213Z","shell.execute_reply":"2025-09-07T18:07:03.474988Z"}},"outputs":[{"name":"stdout","text":"Train Epoch: 36 [0/560 (0%)]\tLoss: 91.890656\nTrain Epoch: 36 [320/560 (56%)]\tLoss: 98.807159\n====> Epoch: 36 Average loss: 100.8290\nEpoch: 36 Average validation loss: 99.9969\nValidation loss decreased. Saving model.\nTrain Epoch: 37 [0/560 (0%)]\tLoss: 102.379784\nTrain Epoch: 37 [320/560 (56%)]\tLoss: 104.977051\n====> Epoch: 37 Average loss: 97.9063\nEpoch: 37 Average validation loss: 97.4970\nValidation loss decreased. Saving model.\nTrain Epoch: 38 [0/560 (0%)]\tLoss: 102.069824\nTrain Epoch: 38 [320/560 (56%)]\tLoss: 91.993797\n====> Epoch: 38 Average loss: 97.3558\nEpoch: 38 Average validation loss: 99.3302\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 39 [0/560 (0%)]\tLoss: 100.653275\nTrain Epoch: 39 [320/560 (56%)]\tLoss: 98.124855\n====> Epoch: 39 Average loss: 94.7040\nEpoch: 39 Average validation loss: 97.7283\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 40 [0/560 (0%)]\tLoss: 79.813614\nTrain Epoch: 40 [320/560 (56%)]\tLoss: 88.038559\n====> Epoch: 40 Average loss: 95.5497\nEpoch: 40 Average validation loss: 109.3431\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 41 [0/560 (0%)]\tLoss: 89.470329\nTrain Epoch: 41 [320/560 (56%)]\tLoss: 90.069450\n====> Epoch: 41 Average loss: 94.8704\nEpoch: 41 Average validation loss: 96.8546\nValidation loss decreased. Saving model.\nTrain Epoch: 42 [0/560 (0%)]\tLoss: 75.071983\nTrain Epoch: 42 [320/560 (56%)]\tLoss: 80.768570\n====> Epoch: 42 Average loss: 92.2838\nEpoch: 42 Average validation loss: 95.3490\nValidation loss decreased. Saving model.\nTrain Epoch: 43 [0/560 (0%)]\tLoss: 103.886833\nTrain Epoch: 43 [320/560 (56%)]\tLoss: 84.983162\n====> Epoch: 43 Average loss: 92.0407\nEpoch: 43 Average validation loss: 97.1793\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 44 [0/560 (0%)]\tLoss: 87.691452\nTrain Epoch: 44 [320/560 (56%)]\tLoss: 93.256142\n====> Epoch: 44 Average loss: 92.7046\nEpoch: 44 Average validation loss: 95.7739\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 45 [0/560 (0%)]\tLoss: 91.244225\nTrain Epoch: 45 [320/560 (56%)]\tLoss: 96.087387\n====> Epoch: 45 Average loss: 92.5209\nEpoch: 45 Average validation loss: 94.7560\nValidation loss decreased. Saving model.\nTrain Epoch: 46 [0/560 (0%)]\tLoss: 100.938187\nTrain Epoch: 46 [320/560 (56%)]\tLoss: 86.744873\n====> Epoch: 46 Average loss: 92.1039\nEpoch: 46 Average validation loss: 94.9272\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 47 [0/560 (0%)]\tLoss: 89.321777\nTrain Epoch: 47 [320/560 (56%)]\tLoss: 80.102997\n====> Epoch: 47 Average loss: 92.0309\nEpoch: 47 Average validation loss: 100.5377\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 48 [0/560 (0%)]\tLoss: 91.760300\nTrain Epoch: 48 [320/560 (56%)]\tLoss: 92.049988\n====> Epoch: 48 Average loss: 93.2760\nEpoch: 48 Average validation loss: 104.2397\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 49 [0/560 (0%)]\tLoss: 98.131775\nTrain Epoch: 49 [320/560 (56%)]\tLoss: 98.680664\n====> Epoch: 49 Average loss: 92.8670\nEpoch: 49 Average validation loss: 93.8963\nValidation loss decreased. Saving model.\nTrain Epoch: 50 [0/560 (0%)]\tLoss: 83.466660\nTrain Epoch: 50 [320/560 (56%)]\tLoss: 93.326981\n====> Epoch: 50 Average loss: 91.0419\nEpoch: 50 Average validation loss: 95.3841\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 51 [0/560 (0%)]\tLoss: 90.013000\nTrain Epoch: 51 [320/560 (56%)]\tLoss: 85.149933\n====> Epoch: 51 Average loss: 90.6448\nEpoch: 51 Average validation loss: 96.0505\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 52 [0/560 (0%)]\tLoss: 95.047615\nTrain Epoch: 52 [320/560 (56%)]\tLoss: 82.871941\n====> Epoch: 52 Average loss: 89.8378\nEpoch: 52 Average validation loss: 97.0968\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 53 [0/560 (0%)]\tLoss: 93.592232\nTrain Epoch: 53 [320/560 (56%)]\tLoss: 87.734741\n====> Epoch: 53 Average loss: 89.7486\nEpoch: 53 Average validation loss: 93.6031\nValidation loss decreased. Saving model.\nTrain Epoch: 54 [0/560 (0%)]\tLoss: 95.840836\nTrain Epoch: 54 [320/560 (56%)]\tLoss: 93.242256\n====> Epoch: 54 Average loss: 88.8927\nEpoch: 54 Average validation loss: 92.8880\nValidation loss decreased. Saving model.\nTrain Epoch: 55 [0/560 (0%)]\tLoss: 77.056229\nTrain Epoch: 55 [320/560 (56%)]\tLoss: 69.128883\n====> Epoch: 55 Average loss: 89.5596\nEpoch: 55 Average validation loss: 94.5106\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 56 [0/560 (0%)]\tLoss: 93.813759\nTrain Epoch: 56 [320/560 (56%)]\tLoss: 99.729462\n====> Epoch: 56 Average loss: 88.9294\nEpoch: 56 Average validation loss: 94.0060\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 57 [0/560 (0%)]\tLoss: 88.689957\nTrain Epoch: 57 [320/560 (56%)]\tLoss: 88.052147\n====> Epoch: 57 Average loss: 87.0460\nEpoch: 57 Average validation loss: 91.7469\nValidation loss decreased. Saving model.\nTrain Epoch: 58 [0/560 (0%)]\tLoss: 86.324974\nTrain Epoch: 58 [320/560 (56%)]\tLoss: 89.514244\n====> Epoch: 58 Average loss: 86.6826\nEpoch: 58 Average validation loss: 95.1931\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 59 [0/560 (0%)]\tLoss: 86.778076\nTrain Epoch: 59 [320/560 (56%)]\tLoss: 89.247559\n====> Epoch: 59 Average loss: 87.2526\nEpoch: 59 Average validation loss: 94.0637\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 60 [0/560 (0%)]\tLoss: 82.658081\nTrain Epoch: 60 [320/560 (56%)]\tLoss: 71.270164\n====> Epoch: 60 Average loss: 87.0439\nEpoch: 60 Average validation loss: 92.1636\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 61 [0/560 (0%)]\tLoss: 83.345551\nTrain Epoch: 61 [320/560 (56%)]\tLoss: 89.813789\n====> Epoch: 61 Average loss: 87.8624\nEpoch: 61 Average validation loss: 90.5078\nValidation loss decreased. Saving model.\nTrain Epoch: 62 [0/560 (0%)]\tLoss: 81.259399\nTrain Epoch: 62 [320/560 (56%)]\tLoss: 77.148315\n====> Epoch: 62 Average loss: 85.4583\nEpoch: 62 Average validation loss: 90.5957\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 63 [0/560 (0%)]\tLoss: 84.805016\nTrain Epoch: 63 [320/560 (56%)]\tLoss: 95.516907\n====> Epoch: 63 Average loss: 85.0070\nEpoch: 63 Average validation loss: 98.0814\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 64 [0/560 (0%)]\tLoss: 86.438263\nTrain Epoch: 64 [320/560 (56%)]\tLoss: 76.670364\n====> Epoch: 64 Average loss: 85.6681\nEpoch: 64 Average validation loss: 92.2970\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 65 [0/560 (0%)]\tLoss: 92.766647\nTrain Epoch: 65 [320/560 (56%)]\tLoss: 75.790298\n====> Epoch: 65 Average loss: 85.1778\nEpoch: 65 Average validation loss: 93.7166\nValidation loss did not improve. Patience: 4/10\nTrain Epoch: 66 [0/560 (0%)]\tLoss: 83.846115\nTrain Epoch: 66 [320/560 (56%)]\tLoss: 75.051392\n====> Epoch: 66 Average loss: 84.0395\nEpoch: 66 Average validation loss: 91.4576\nValidation loss did not improve. Patience: 5/10\nTrain Epoch: 67 [0/560 (0%)]\tLoss: 83.819778\nTrain Epoch: 67 [320/560 (56%)]\tLoss: 76.822754\n====> Epoch: 67 Average loss: 82.5831\nEpoch: 67 Average validation loss: 90.5267\nValidation loss did not improve. Patience: 6/10\nTrain Epoch: 68 [0/560 (0%)]\tLoss: 82.789917\nTrain Epoch: 68 [320/560 (56%)]\tLoss: 81.283691\n====> Epoch: 68 Average loss: 84.4080\nEpoch: 68 Average validation loss: 90.7116\nValidation loss did not improve. Patience: 7/10\nTrain Epoch: 69 [0/560 (0%)]\tLoss: 82.930428\nTrain Epoch: 69 [320/560 (56%)]\tLoss: 82.993462\n====> Epoch: 69 Average loss: 82.9951\nEpoch: 69 Average validation loss: 89.4760\nValidation loss decreased. Saving model.\nTrain Epoch: 70 [0/560 (0%)]\tLoss: 67.640739\nTrain Epoch: 70 [320/560 (56%)]\tLoss: 69.417358\n====> Epoch: 70 Average loss: 84.3170\nEpoch: 70 Average validation loss: 91.2600\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 71 [0/560 (0%)]\tLoss: 76.677986\nTrain Epoch: 71 [320/560 (56%)]\tLoss: 74.090454\n====> Epoch: 71 Average loss: 82.9609\nEpoch: 71 Average validation loss: 88.7091\nValidation loss decreased. Saving model.\nTrain Epoch: 72 [0/560 (0%)]\tLoss: 84.036598\nTrain Epoch: 72 [320/560 (56%)]\tLoss: 72.982933\n====> Epoch: 72 Average loss: 81.9718\nEpoch: 72 Average validation loss: 88.6864\nValidation loss decreased. Saving model.\nTrain Epoch: 73 [0/560 (0%)]\tLoss: 77.621384\nTrain Epoch: 73 [320/560 (56%)]\tLoss: 71.162117\n====> Epoch: 73 Average loss: 82.9475\nEpoch: 73 Average validation loss: 89.0157\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 74 [0/560 (0%)]\tLoss: 75.656471\nTrain Epoch: 74 [320/560 (56%)]\tLoss: 68.292397\n====> Epoch: 74 Average loss: 81.5342\nEpoch: 74 Average validation loss: 89.7906\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 75 [0/560 (0%)]\tLoss: 74.135544\nTrain Epoch: 75 [320/560 (56%)]\tLoss: 79.708923\n====> Epoch: 75 Average loss: 81.4119\nEpoch: 75 Average validation loss: 91.7681\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 76 [0/560 (0%)]\tLoss: 77.917175\nTrain Epoch: 76 [320/560 (56%)]\tLoss: 83.661537\n====> Epoch: 76 Average loss: 81.7149\nEpoch: 76 Average validation loss: 88.7525\nValidation loss did not improve. Patience: 4/10\nTrain Epoch: 77 [0/560 (0%)]\tLoss: 89.023598\nTrain Epoch: 77 [320/560 (56%)]\tLoss: 73.754761\n====> Epoch: 77 Average loss: 81.9829\nEpoch: 77 Average validation loss: 90.2380\nValidation loss did not improve. Patience: 5/10\nTrain Epoch: 78 [0/560 (0%)]\tLoss: 73.514908\nTrain Epoch: 78 [320/560 (56%)]\tLoss: 80.367989\n====> Epoch: 78 Average loss: 81.3414\nEpoch: 78 Average validation loss: 90.4692\nValidation loss did not improve. Patience: 6/10\nTrain Epoch: 79 [0/560 (0%)]\tLoss: 89.920425\nTrain Epoch: 79 [320/560 (56%)]\tLoss: 90.666748\n====> Epoch: 79 Average loss: 81.4910\nEpoch: 79 Average validation loss: 88.2470\nValidation loss decreased. Saving model.\nTrain Epoch: 80 [0/560 (0%)]\tLoss: 78.383926\nTrain Epoch: 80 [320/560 (56%)]\tLoss: 90.149429\n====> Epoch: 80 Average loss: 81.3445\nEpoch: 80 Average validation loss: 90.3060\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 81 [0/560 (0%)]\tLoss: 80.533417\nTrain Epoch: 81 [320/560 (56%)]\tLoss: 84.635468\n====> Epoch: 81 Average loss: 81.1636\nEpoch: 81 Average validation loss: 92.7245\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 82 [0/560 (0%)]\tLoss: 81.619110\nTrain Epoch: 82 [320/560 (56%)]\tLoss: 81.127098\n====> Epoch: 82 Average loss: 81.4231\nEpoch: 82 Average validation loss: 90.4939\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 83 [0/560 (0%)]\tLoss: 72.410927\nTrain Epoch: 83 [320/560 (56%)]\tLoss: 68.642517\n====> Epoch: 83 Average loss: 79.4656\nEpoch: 83 Average validation loss: 88.5703\nValidation loss did not improve. Patience: 4/10\nTrain Epoch: 84 [0/560 (0%)]\tLoss: 78.878433\nTrain Epoch: 84 [320/560 (56%)]\tLoss: 77.457718\n====> Epoch: 84 Average loss: 79.6763\nEpoch: 84 Average validation loss: 87.4530\nValidation loss decreased. Saving model.\nTrain Epoch: 85 [0/560 (0%)]\tLoss: 70.217361\nTrain Epoch: 85 [320/560 (56%)]\tLoss: 83.609306\n====> Epoch: 85 Average loss: 79.7211\nEpoch: 85 Average validation loss: 90.1696\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 86 [0/560 (0%)]\tLoss: 95.726494\nTrain Epoch: 86 [320/560 (56%)]\tLoss: 78.129471\n====> Epoch: 86 Average loss: 80.5116\nEpoch: 86 Average validation loss: 91.4403\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 87 [0/560 (0%)]\tLoss: 76.541656\nTrain Epoch: 87 [320/560 (56%)]\tLoss: 76.566376\n====> Epoch: 87 Average loss: 79.4410\nEpoch: 87 Average validation loss: 91.4831\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 88 [0/560 (0%)]\tLoss: 78.871391\nTrain Epoch: 88 [320/560 (56%)]\tLoss: 80.709030\n====> Epoch: 88 Average loss: 78.4832\nEpoch: 88 Average validation loss: 87.2450\nValidation loss decreased. Saving model.\nTrain Epoch: 89 [0/560 (0%)]\tLoss: 80.654411\nTrain Epoch: 89 [320/560 (56%)]\tLoss: 81.725685\n====> Epoch: 89 Average loss: 79.1002\nEpoch: 89 Average validation loss: 89.4145\nValidation loss did not improve. Patience: 1/10\nTrain Epoch: 90 [0/560 (0%)]\tLoss: 79.435013\nTrain Epoch: 90 [320/560 (56%)]\tLoss: 74.082474\n====> Epoch: 90 Average loss: 78.6033\nEpoch: 90 Average validation loss: 90.5820\nValidation loss did not improve. Patience: 2/10\nTrain Epoch: 91 [0/560 (0%)]\tLoss: 67.435677\nTrain Epoch: 91 [320/560 (56%)]\tLoss: 77.160484\n====> Epoch: 91 Average loss: 78.5316\nEpoch: 91 Average validation loss: 88.7315\nValidation loss did not improve. Patience: 3/10\nTrain Epoch: 92 [0/560 (0%)]\tLoss: 64.796700\nTrain Epoch: 92 [320/560 (56%)]\tLoss: 73.685501\n====> Epoch: 92 Average loss: 78.0068\nEpoch: 92 Average validation loss: 89.4180\nValidation loss did not improve. Patience: 4/10\nTrain Epoch: 93 [0/560 (0%)]\tLoss: 74.313622\nTrain Epoch: 93 [320/560 (56%)]\tLoss: 81.155991\n====> Epoch: 93 Average loss: 80.3259\nEpoch: 93 Average validation loss: 87.4191\nValidation loss did not improve. Patience: 5/10\nTrain Epoch: 94 [0/560 (0%)]\tLoss: 74.442307\nTrain Epoch: 94 [320/560 (56%)]\tLoss: 88.336830\n====> Epoch: 94 Average loss: 78.0935\nEpoch: 94 Average validation loss: 89.8706\nValidation loss did not improve. Patience: 6/10\nTrain Epoch: 95 [0/560 (0%)]\tLoss: 80.495331\nTrain Epoch: 95 [320/560 (56%)]\tLoss: 82.175087\n====> Epoch: 95 Average loss: 80.1516\nEpoch: 95 Average validation loss: 91.2477\nValidation loss did not improve. Patience: 7/10\nTrain Epoch: 96 [0/560 (0%)]\tLoss: 67.721924\nTrain Epoch: 96 [320/560 (56%)]\tLoss: 82.465195\n====> Epoch: 96 Average loss: 77.7845\nEpoch: 96 Average validation loss: 92.5095\nValidation loss did not improve. Patience: 8/10\nTrain Epoch: 97 [0/560 (0%)]\tLoss: 74.155190\nTrain Epoch: 97 [320/560 (56%)]\tLoss: 69.036079\n====> Epoch: 97 Average loss: 77.4772\nEpoch: 97 Average validation loss: 90.3163\nValidation loss did not improve. Patience: 9/10\nTrain Epoch: 98 [0/560 (0%)]\tLoss: 62.986778\nTrain Epoch: 98 [320/560 (56%)]\tLoss: 78.562355\n====> Epoch: 98 Average loss: 78.1447\nEpoch: 98 Average validation loss: 91.0415\nValidation loss did not improve. Patience: 10/10\nEarly stopping triggered!\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from torchvision.utils import save_image\n\ndef test2(epoch):\n    model2.eval()\n    with torch.no_grad():\n        for i, data in enumerate(train_loader):\n            data = data.to(device)\n            recon_batch, mu, logvar = model2(data)\n\n            if i == 0:\n                n = min(data.size(0), 8)   # take up to 8 samples\n\n                # concat original and reconstructions along the batch dimension\n                comparison = torch.cat([data[:n],\n                                        recon_batch[:n]])\n\n                save_image(comparison.cpu(),\n                           f\"/kaggle/working/reconstruction2_{epoch}.png\",\n                           nrow=n)\n            break   # only first batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:45:24.778602Z","iopub.execute_input":"2025-09-07T17:45:24.779303Z","iopub.status.idle":"2025-09-07T17:45:24.783974Z","shell.execute_reply.started":"2025-09-07T17:45:24.779279Z","shell.execute_reply":"2025-09-07T17:45:24.783216Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"test2(98)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T18:07:34.868514Z","iopub.execute_input":"2025-09-07T18:07:34.868764Z","iopub.status.idle":"2025-09-07T18:07:36.147772Z","shell.execute_reply.started":"2025-09-07T18:07:34.868747Z","shell.execute_reply":"2025-09-07T18:07:36.146997Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"**Results: Check Img3 on GITHUB**","metadata":{}},{"cell_type":"markdown","source":"**Comments:** The performance of this model is significantly better than the last 2. So this is the best model among all the three.","metadata":{}},{"cell_type":"markdown","source":"**Hypothesis:**\n1. A larger embedding space could improve perfomance even more but computationally required will be more.","metadata":{}},{"cell_type":"markdown","source":"\n**Conclusion: Introducing a beta factor to the KL loss and increasing the model complexity can definitely improve model perfomance**\n","metadata":{}}]}